{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Random seed\n",
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Data using pandas\n",
    "Corpus = pd.read_parquet('/Users/xiomara/Desktop/BSAN 6070/Teaching Workshop/test-00000-of-00001.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step - 1: Data Pre-processing - This will help in getting better results through the classification algorithms\n",
    "\n",
    "# Step - 1a : Remove blank rows if any.\n",
    "Corpus['text'].dropna(inplace=True)\n",
    "\n",
    "# Step - 1b : Change all the text to lower case. This is required as python interprets 'dog' and 'DOG' differently\n",
    "Corpus['text'] = [entry.lower() for entry in Corpus['text']]\n",
    "\n",
    "# Step - 1c : Tokenization : In this each entry in the corpus will be broken into set of words\n",
    "Corpus['text']= [word_tokenize(entry) for entry in Corpus['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step - 1d : Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting.\n",
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label  \\\n",
      "0  [i, love, sci-fi, and, am, willing, to, put, u...      0   \n",
      "1  [worth, the, entertainment, value, of, a, rent...      0   \n",
      "2  [its, a, totally, average, film, with, a, few,...      0   \n",
      "3  [star, rating, :, *, *, *, *, *, saturday, nig...      0   \n",
      "4  [first, off, let, me, say, ,, if, you, have, n...      0   \n",
      "\n",
      "                                          text_final  \n",
      "0  love willing put lot usually underfunded misun...  \n",
      "1  worth entertainment value rental especially li...  \n",
      "2  totally average film action sequence make plot...  \n",
      "3  star rating saturday night friday night friday...  \n",
      "4  first let say enjoy van damme movie since bloo...  \n"
     ]
    }
   ],
   "source": [
    "# Initialize WordNetLemmatizer outside the loop\n",
    "word_Lemmatized = WordNetLemmatizer()\n",
    "\n",
    "# Convert stopwords to a set for faster lookup\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to process text\n",
    "def process_text(entry):\n",
    "    final_words = []\n",
    "        # Ensure entry is a string and split into words\n",
    "    for word, tag in pos_tag(entry):  \n",
    "            # Check for stop words and only alphabets\n",
    "        if word not in stop_words and word.isalpha():\n",
    "            word_final = word_Lemmatized.lemmatize(word, tag_map[tag[0]])\n",
    "            final_words.append(word_final)\n",
    "        # Return the processed text as a string\n",
    "    return ' '.join(final_words)\n",
    "    \n",
    "# Apply the process_text function to handle text processing\n",
    "Corpus['text_final'] = Corpus['text'].apply(process_text)\n",
    "\n",
    "# Print the first few entries of the processed text\n",
    "print(Corpus.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step - 2: Split the model into Train and Test Data set\n",
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus['text_final'],Corpus['label'],test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step - 3: Label encode the target variable  - This is done to transform Categorical data of string type in the data set into numerical values\n",
    "Encoder = LabelEncoder()\n",
    "Train_Y = Encoder.fit_transform(Train_Y)\n",
    "Test_Y = Encoder.fit_transform(Test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the lists of words to strings by joining them with spaces for Train_X and Test_X\n",
    "Train_X = Train_X.apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "Test_X = Test_X.apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "# Create a TfidfVectorizer with a maximum of 5000 features\n",
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit the TF-IDF vectorizer on the training data\n",
    "Tfidf_vect.fit(Train_X)\n",
    "\n",
    "# Transform the training and test data\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  85.52\n"
     ]
    }
   ],
   "source": [
    "# Step - 5: Now we can run different algorithms to classify out data check for accuracy\n",
    "\n",
    "# Classifier - Algorithm - Naive Bayes\n",
    "# fit the training dataset on the classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Tfidf,Train_Y)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
